{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlYUTDSRBlgr",
        "outputId": "3ef135c6-3084-45e6-ebab-c91e1bd62cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCCpfPylhxwE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from memory_profiler import memory_usage\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "import psutil\n",
        "# import resource\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve, auc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the image size\n",
        "import os\n",
        "IMG_SIZE = 50\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab-Notebooks/Anomaly-Detection/CSV\n",
        "\n",
        "train_df = pd.read_csv('./train_index_small.csv')\n",
        "test_df = pd.read_csv('./test_index_small.csv')\n",
        "# train_df = pd.read_csv('./train_final_index_2.csv')\n",
        "# test_df = pd.read_csv('./test_final_index_2.csv')\n",
        "# train_df = pd.read_csv('./train_final_index.csv')\n",
        "# test_df = pd.read_csv('./test_final_index.csv')\n",
        "# test_df = pd.read_csv('./val_final_index.csv')\n",
        "# train_df = pd.read_csv('./train_feature_index.csv')\n",
        "# test_df = pd.read_csv('./test_feature_index.csv')\n",
        "# test_df = pd.read_csv('./val_feature_index.csv')\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab-Notebooks/Anomaly-Detection/fiveTwelve\"\n",
        "print(\"found\", os.path.exists(path=path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI2ZyhiWANpD",
        "outputId": "1cfce00b-fd46-4484-cfb2-abe7b9420951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab-Notebooks/Anomaly-Detection/CSV\n",
            "found True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.drop(columns=[\"label\", \"path\"])\n",
        "y_train = train_df[\"label\"]\n",
        "\n",
        "X_test = test_df.drop(columns=[\"label\", \"path\"])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    image = image / 255.0\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "-4OtXaUiAsLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the memory code after testing\n",
        "# Sample 1\n",
        "process = psutil.Process()\n",
        "before_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
        "print(\"process\", process, \"before Memory\", before_memory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntqZLKJGA2uy",
        "outputId": "b7cb2c7b-e3ea-40a9-8339-7690f050575d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "process psutil.Process(pid=528, name='python3', status='running', started='21:25:58') before Memory 584.67578125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "X_train_reorder = np.random.permutation(X_train)\n",
        "knn = KNeighborsClassifier()\n",
        "params_knn = {\"n_neighbors\": np.arange(1, 25)}\n",
        "knn_gs = GridSearchCV(knn, params_knn, cv=10)\n",
        "knn_gs.fit(X_train, y_train)\n",
        "knn_gs.fit(X_train_reorder, y_train)\n",
        "\n",
        "best_params = knn_gs.best_params_\n",
        "best_score = knn_gs.best_score_\n",
        "\n",
        "# print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "used_algorithm = knn.algorithm\n",
        "\n",
        "knn_best = knn_gs.best_estimator_\n",
        "print(knn_gs.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhQCmGCHA36a",
        "outputId": "028e007a-e591-4ba1-a9cf-7a6220550e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Score: 0.54\n",
            "{'n_neighbors': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "X_train_reorder = np.random.permutation(X_train)\n",
        "rf = RandomForestClassifier()\n",
        "params_rf = {\"n_estimators\": [50, 100, 200]}\n",
        "rf_gs = GridSearchCV(rf, params_rf, cv=10)\n",
        "rf_gs.fit(X_train, y_train)\n",
        "rf_gs.fit(X_train_reorder, y_train)\n",
        "\n",
        "\n",
        "rf_best = rf_gs.best_estimator_\n",
        "print(rf_gs.best_params_)\n",
        "rf_best_params = rf_gs.best_params_\n",
        "rf_best_score = rf_gs.best_score_\n",
        "\n",
        "# print(\"Best Parameters:\", rf_best_params)\n",
        "print(\"Best Score:\", rf_best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idhExYo3A_l_",
        "outputId": "c1889804-2087-4d94-ba18-b2071f743cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': 100}\n",
            "Best Score: 0.5285714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"knn: {}\".format(knn_best.score(X_test, y_test)))\n",
        "print(\"rf: {}\".format(rf_best.score(X_test, y_test)))\n",
        "\n",
        "estimators=[(\"knn\", knn_best), (\"rf\", rf_best)]\n",
        "ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "# ensemble = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "ens_best = ensemble.score(X_test, y_test)\n",
        "\n",
        "print(\"Ens_best\", ens_best)\n",
        "\n",
        "y_pred = ensemble.predict(X_test)\n",
        "\n",
        "y_pred = ensemble.predict(X_test)\n",
        "\n",
        "y_pred_labels = y_pred\n",
        "\n",
        "ens_labels = ensemble.predict(X_test)\n",
        "\n",
        "score1 = knn_best.score(X_test, y_test)\n",
        "score2 = rf_best.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "J6h80_sIBIQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f5682f-9a8b-43bf-b14c-f8d703746ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knn: 0.5955555555555555\n",
            "rf: 0.6066666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ens_best 0.8777777777777778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def image_generator(X, y, batch_size=32):\n",
        "    num_samples = len(X)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        batch_indices = []\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            end = min(start + batch_size, num_samples)\n",
        "            batch_indices.extend(indices[start:end])\n",
        "\n",
        "            if len(batch_indices) == batch_size:\n",
        "                batch_X = np.zeros((batch_size, 224, 224, 3))\n",
        "                batch_y = np.zeros((batch_size,))\n",
        "                for i, idx in enumerate(batch_indices):\n",
        "                    if idx >= num_samples:\n",
        "                        continue\n",
        "\n",
        "                    if \"path\" not in X.iloc[idx]:\n",
        "                        continue\n",
        "                    path = X.iloc[idx][\"path\"]\n",
        "                    image = load_and_preprocess_image(path)\n",
        "                    if image is None:\n",
        "                        continue\n",
        "                    label = y.iloc[idx]\n",
        "                    X_modified = X.drop('path', axis=1)\n",
        "                    X_modified['path_id'] = range(1, len(X_modified) + 1)\n",
        "\n",
        "                    input_data = pd.DataFrame([X_modified.iloc[idx]])\n",
        "                    prediction = ensemble.predict(input_data)\n",
        "\n",
        "                    batch_X[i] = image\n",
        "                    batch_y[i] = prediction\n",
        "                yield batch_X, batch_y\n",
        "                batch_indices = []\n",
        "\n",
        "        # If there are remaining samples that don't form a full batch, process them as well\n",
        "        if batch_indices:\n",
        "            batch_X = np.zeros((len(batch_indices), 224, 224, 3))\n",
        "            batch_y = np.zeros((len(batch_indices),))\n",
        "            for i, idx in enumerate(batch_indices):\n",
        "                if idx >= num_samples:\n",
        "                    continue\n",
        "                if \"path\" not in X.iloc[idx]:\n",
        "                    continue\n",
        "                path = X.iloc[idx][\"path\"]\n",
        "                image = load_and_preprocess_image(path)\n",
        "                if image is None:\n",
        "                    continue\n",
        "                label = y.iloc[idx]\n",
        "                X_modified = X.drop('path', axis=1)\n",
        "                X_modified['path_id'] = range(1, len(X_modified) + 1)\n",
        "\n",
        "                input_data = pd.DataFrame([X_modified.iloc[idx]])\n",
        "                prediction = ensemble.predict(input_data)\n",
        "\n",
        "                batch_X[i] = image\n",
        "                batch_y[i] = prediction\n",
        "            yield batch_X, batch_y\n",
        "\n"
      ],
      "metadata": {
        "id": "BXLjGWPVBS70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_generator(X, y, batch_size=32):\n",
        "    num_samples = len(X)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        batch_indices = []\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            end = min(start + batch_size, num_samples)\n",
        "            batch_indices.extend(indices[start:end])\n",
        "\n",
        "            if len(batch_indices) == batch_size:\n",
        "                batch_X = np.zeros((batch_size, 224, 224, 3))\n",
        "                batch_y = np.zeros((batch_size,))  # Use num_classes from your problem\n",
        "\n",
        "                for i, idx in enumerate(batch_indices):\n",
        "                    if idx >= num_samples:\n",
        "                        continue  # Skip this sample\n",
        "\n",
        "                    if \"path\" not in X.iloc[idx]:\n",
        "                        continue  # Skip this sample if 'path' key is missing\n",
        "\n",
        "                    path = X.iloc[idx][\"path\"]\n",
        "                    image = load_and_preprocess_image(path)\n",
        "                    if image is None:\n",
        "                        continue  # Skip this sample\n",
        "\n",
        "                    batch_X[i] = image\n",
        "                    batch_y[i] = y.iloc[idx]\n",
        "\n",
        "                yield batch_X, batch_y\n",
        "                batch_indices = []\n",
        "\n",
        "        # If there are remaining samples that don't form a full batch, process them as well\n",
        "        if batch_indices:\n",
        "            batch_X = np.zeros((len(batch_indices), 224, 224, 3))\n",
        "            batch_y = np.zeros((len(batch_indices), 10))  # Use num_classes from your problem\n",
        "            for i, idx in enumerate(batch_indices):\n",
        "                if idx >= num_samples:\n",
        "                    continue  # Skip this sample\n",
        "\n",
        "                if \"path\" not in X.iloc[idx]:\n",
        "                    continue  # Skip this sample if 'path' key is missing\n",
        "\n",
        "                path = X.iloc[idx][\"path\"]\n",
        "                image = load_and_preprocess_image(path)\n",
        "                if image is None:\n",
        "                    continue  # Skip this sample\n",
        "\n",
        "                batch_X[i] = image\n",
        "                batch_y[i] = y.iloc[idx]\n",
        "\n",
        "            yield batch_X, batch_y\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = cv2.imread(path)\n",
        "    if(image is not None):\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      image = cv2.resize(image, (224, 224))\n",
        "      image = image / 255.0\n",
        "    return image\n",
        "\n",
        "X_train = train_df.drop(columns=[\"label\", \"path_id\"])\n",
        "y_train = train_df[\"label\"]\n",
        "\n",
        "X_test = test_df.drop(columns=[\"label\", \"path_id\"])\n",
        "y_test = test_df[\"label\"]\n",
        "# print(\"X_test\", X_test, \"y_test\", y_test)"
      ],
      "metadata": {
        "id": "jgNn_cMP_2rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "def run_code():\n",
        "    base_model = EfficientNetB7(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=(224, 224, 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    # x = Dropout(0.2)(x)\n",
        "    outputs = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    train_generator = image_generator(X_train, y_train, len(y_train))\n",
        "    test_generator = image_generator(X_test, y_test, len(y_test))\n",
        "    # train_generator = image_generator(X_train, y_train, 500)\n",
        "    # test_generator = image_generator(X_test, y_test, 500)\n",
        "\n",
        "    X_train_processed, y_train_processed = next(train_generator)\n",
        "    X_test_processed, y_test_processed = next(test_generator)\n",
        "    # print(\"X_test_processed\", len(X_test_processed), \"y_test_processed\", len(y_test_processed))\n",
        "\n",
        "    epochs = 5\n",
        "    accuracy_history = []\n",
        "    loss_history = []\n",
        "    val_accuracy_history=[]\n",
        "    val_loss_history=[]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        history = model.fit(X_train_processed, y_train_processed, epochs=1, validation_data=(X_test_processed, y_test_processed))\n",
        "        batch_size = 32\n",
        "        training_accuracy = history.history['accuracy'][0]\n",
        "        training_loss = history.history['loss'][0]\n",
        "        val_training_accuracy=history.history['val_accuracy'][0]\n",
        "        val_training_loss=history.history['val_loss'][0]\n",
        "\n",
        "        accuracy_history.append(training_accuracy)\n",
        "        loss_history.append(training_loss)\n",
        "        val_accuracy_history.append(val_training_accuracy)\n",
        "        val_loss_history.append(val_training_loss)\n",
        "\n",
        "    print(\"Completed\")\n",
        "    print(\"accuracy_history:\", accuracy_history)\n",
        "    print(\"val_accuracy_history:\", val_accuracy_history)\n",
        "    print(\"loss_history:\", loss_history)\n",
        "    print(\"val_loss_history:\", val_loss_history)\n",
        "    print(\"**************************************\")\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_test_processed, y_test_processed)\n",
        "    print(\"Test Loss:\",test_loss, \"Test accuracy:\", test_acc)\n",
        "    # Check the shape and data type of y_train_processed and y_test_processed\n",
        "\n",
        "    # Assuming y_test_processed contains binary labels (0 and 1)\n",
        "    y_pred = model.predict(X_test_processed)\n",
        "    y_pred_labels = y_pred.argmax(axis=1) # Convert probabilities to binary labels\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(y_test_processed, y_pred_labels)\n",
        "    print(report)\n",
        "    # Convert the data type of y_test_processed to integer\n",
        "    y_test_processed = y_test_processed.astype(int)\n",
        "\n",
        "    # Get predicted labels from the model\n",
        "    y_pred = model.predict(X_test_processed)\n",
        "    # print(\"y_pred\", y_pred, y_test_processed)\n",
        "    y_pred_labels = y_pred.argmax(axis=1)\n",
        "\n",
        "    # Generate the confusion matrix\n",
        "    cm = confusion_matrix(y_test_processed, y_pred_labels)\n",
        "    print(\"confusion_matrix\", cm)\n",
        "    # Calculate the F1 score\n",
        "    f1 = f1_score(y_test_processed, y_pred_labels, average='macro')\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "    # Apply the chosen threshold to classify instances\n",
        "    y_pred_labels = (y_pred > best_threshold).astype(int)\n",
        "    threshold = 0.5\n",
        "\n",
        "    # Get predicted labels from the model\n",
        "    y_pred = model.predict(X_test_processed)\n",
        "\n",
        "    # Convert the predicted probabilities to binary labels based on the threshold\n",
        "    y_pred_labels = (y_pred[:, 1] > threshold).astype(int)  # Assuming anomaly class is the positive class (index 1)\n",
        "\n",
        "    # Count the number of anomalies detected\n",
        "    anomalies_detected = sum(y_pred_labels)\n",
        "\n",
        "    # Calculate the percentage of anomalies detected\n",
        "    percentage_detected = (anomalies_detected / len(y_test_processed)) * 100\n",
        "\n",
        "    print(\"Anomalies Detected:\", anomalies_detected)\n",
        "    print(\"Percentage of Anomalies Detected:\", percentage_detected, \"%\")\n",
        "\n",
        "    # Plot exponential accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epochs + 1), accuracy_history, linestyle='-', label='Training Accuracy')\n",
        "    plt.plot(range(1, epochs + 1), val_accuracy_history, linestyle='-', label='Testing Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(False)\n",
        "    plt.xticks(np.arange(1, epochs + 1))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot exponential loss\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epochs + 1), loss_history, linestyle='-', label='Training Loss')\n",
        "    plt.plot(range(1, epochs + 1), val_loss_history, linestyle='-', label='Testing Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(False)\n",
        "    plt.xticks(np.arange(1, epochs + 1))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test_processed, y_pred[:, 1])\n",
        "\n",
        "    # Calculate area under the precision-recall curve (AUC-PR)\n",
        "    auc_pr = auc(recall, precision)\n",
        "    print(\"Precision-Recall curve (AUC) = \", auc_pr)\n",
        "    # print(\"Precision, Recall:\", precision, recall)\n",
        "\n",
        "    # Plot precision-recall curve\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "    return 1\n"
      ],
      "metadata": {
        "id": "7mIY_dJWoltK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_code()\n",
        "after_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
        "memory_usage = after_memory - before_memory\n",
        "\n",
        "print(\"Memory usage:\", memory_usage, \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ql1tyNXTG6",
        "outputId": "9849b4a5-bce7-4649-c2af-b19a8d183635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258076736/258076736 [==============================] - 1s 0us/step\n",
            " 6/11 [===============>..............] - ETA: 1:34 - loss: 1.0233 - accuracy: 0.5365"
          ]
        }
      ]
    }
  ]
}