{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHLyNMTm0aTl",
        "outputId": "dfc44599-8d9d-41e2-d5e6-e6a315b63bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n"
          ]
        }
      ],
      "source": [
        "pip install memory_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWZviPIa0y4T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from memory_profiler import memory_usage\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "import psutil\n",
        "# import resource\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIlLmt8S05Fw",
        "outputId": "f9d26ba8-27b2-49c6-bbb3-4c5b5364bad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/Anomaly Detection/CSV\n",
            "found True\n"
          ]
        }
      ],
      "source": [
        "# Define the image size\n",
        "import os\n",
        "IMG_SIZE = 50\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/Anomaly Detection/CSV\n",
        "\n",
        "\n",
        "train1_df = pd.read_csv(\"./train_feature_index.csv\")\n",
        "test1_df = pd.read_csv(\"./test_feature_index.csv\")\n",
        "\n",
        "train1_df = train1_df.dropna(axis=0, how='all').iloc[:-2]\n",
        "test1_df = test1_df.dropna(axis=0, how='all').iloc[:-2]\n",
        "\n",
        "train1_df.to_csv(\"train_cleaned.csv\", index=False)\n",
        "test1_df.to_csv(\"test_cleaned.csv\", index=False)\n",
        "\n",
        "train_df = pd.read_csv(\"train_cleaned.csv\")\n",
        "test_df = pd.read_csv(\"test_cleaned.csv\")\n",
        "\n",
        "# train_df = pd.read_csv(\"./train_final_index.csv\")\n",
        "# test_df = pd.read_csv(\"./test_final_index.csv\")\n",
        "# # train_df = pd.read_csv(\"./train_feature_index_1.csv\")\n",
        "# # test_df = pd.read_csv(\"./test_feature_index_1.csv\")\n",
        "\n",
        "# train_df = pd.read_csv(\"./train_feature_index_2.csv\")\n",
        "# test_df = pd.read_csv(\"./test_feature_index_2.csv\")\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Anomaly Detection/fiveTwelve\"\n",
        "print(\"found\", os.path.exists(path=path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00fBP3B_1D_Y"
      },
      "outputs": [],
      "source": [
        "X_train = train_df.drop(columns=[\"label\", \"path\"])\n",
        "y_train = train_df[\"label\"]\n",
        "\n",
        "X_test = test_df.drop(columns=[\"label\", \"path\"])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    image = image / 255.0\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojqv2bRd1FCL",
        "outputId": "fe0a033d-b1b5-4522-a6e0-0a54da4aea5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "process psutil.Process(pid=142, name='python3', status='running', started='09:37:55') before Memory 650.28515625\n"
          ]
        }
      ],
      "source": [
        "#Remove the memory code after testing\n",
        "# Sample 1\n",
        "process = psutil.Process()\n",
        "before_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
        "print(\"process\", process, \"before Memory\", before_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmfQhZoP1HrT",
        "outputId": "c11b6a76-ff9e-483d-b4a3-41f691e1e69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'n_neighbors': 8}\n",
            "Best Score: 0.9788859748803633\n",
            "{'n_neighbors': 8}\n",
            "{'n_estimators': 100}\n",
            "Best Parameters: {'n_estimators': 100}\n",
            "Best Score: 0.9788859748803633\n",
            "X_train shape: (46462, 11)\n",
            "X_test shape: (46462, 11)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "knn: 0.9788859713314106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rf: 0.9586759071929749\n",
            "Ens_best 0.9845680340923766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X_train_reorder = np.random.permutation(X_train)\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "params_knn = {\"n_neighbors\": np.arange(1, 25)}\n",
        "\n",
        "knn_gs = GridSearchCV(knn, params_knn, cv=10)\n",
        "\n",
        "knn_gs.fit(X_train, y_train)\n",
        "knn_gs.fit(X_train_reorder, y_train)\n",
        "\n",
        "best_params = knn_gs.best_params_\n",
        "best_score = knn_gs.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "used_algorithm = knn.algorithm\n",
        "\n",
        "knn_best = knn_gs.best_estimator_\n",
        "print(knn_gs.best_params_)\n",
        "\n",
        "\n",
        "X_train_reorder = np.random.permutation(X_train)\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "params_rf = {\"n_estimators\": [50, 100, 200]}\n",
        "rf_gs = GridSearchCV(rf, params_rf, cv=10)\n",
        "rf_gs.fit(X_train, y_train)\n",
        "rf_gs.fit(X_train_reorder, y_train)\n",
        "\n",
        "\n",
        "rf_best = rf_gs.best_estimator_\n",
        "print(rf_gs.best_params_)\n",
        "rf_best_params = rf_gs.best_params_\n",
        "rf_best_score = rf_gs.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", rf_best_params)\n",
        "print(\"Best Score:\", rf_best_score)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "print(\"knn: {}\".format(knn_best.score(X_test, y_test)))\n",
        "print(\"rf: {}\".format(rf_best.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "estimators=[(\"knn\", knn_best), (\"rf\", rf_best)]\n",
        "ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "# ensemble = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "ens_best = ensemble.score(X_test, y_test)\n",
        "\n",
        "print(\"Ens_best\", ens_best)\n",
        "\n",
        "y_pred = ensemble.predict(X_test)\n",
        "\n",
        "y_pred_labels = y_pred\n",
        "\n",
        "ens_labels = ensemble.predict(X_test)\n",
        "\n",
        "score1 = knn_best.score(X_test, y_test)\n",
        "score2 = rf_best.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIFMkvH22E0u"
      },
      "outputs": [],
      "source": [
        "def image_generator(X, y, batch_size=16):\n",
        "    num_samples = len(X)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        batch_indices = []\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            end = min(start + batch_size, num_samples)\n",
        "            batch_indices.extend(indices[start:end])\n",
        "\n",
        "            if len(batch_indices) == batch_size:\n",
        "                batch_X = np.zeros((batch_size, 224, 224, 3))\n",
        "                batch_y = np.zeros((batch_size,))  # Use num_classes from your problem\n",
        "\n",
        "                for i, idx in enumerate(batch_indices):\n",
        "                    if idx >= num_samples:\n",
        "                        continue  # Skip this sample\n",
        "\n",
        "                    if \"path\" not in X.iloc[idx]:\n",
        "                        continue  # Skip this sample if 'path' key is missing\n",
        "\n",
        "                    path = X.iloc[idx][\"path\"]\n",
        "                    image = load_and_preprocess_image(path)\n",
        "                    if image is None:\n",
        "                        continue  # Skip this sample\n",
        "\n",
        "                    batch_X[i] = image\n",
        "                    batch_y[i] = y.iloc[idx]\n",
        "\n",
        "                yield batch_X, batch_y\n",
        "                batch_indices = []\n",
        "\n",
        "        # If there are remaining samples that don't form a full batch, process them as well\n",
        "        if batch_indices:\n",
        "            batch_X = np.zeros((len(batch_indices), 224, 224, 3))\n",
        "            batch_y = np.zeros((len(batch_indices), 10))  # Use num_classes from your problem\n",
        "            for i, idx in enumerate(batch_indices):\n",
        "                if idx >= num_samples:\n",
        "                    continue  # Skip this sample\n",
        "\n",
        "                if \"path\" not in X.iloc[idx]:\n",
        "                    continue  # Skip this sample if 'path' key is missing\n",
        "\n",
        "                path = X.iloc[idx][\"path\"]\n",
        "                image = load_and_preprocess_image(path)\n",
        "                if image is None:\n",
        "                    continue  # Skip this sample\n",
        "\n",
        "                batch_X[i] = image\n",
        "                batch_y[i] = y.iloc[idx]\n",
        "\n",
        "            yield batch_X, batch_y\n",
        "\n",
        "# def image_generator(X, y, batch_size=32):\n",
        "#     num_samples = len(X)\n",
        "#     indices = np.arange(num_samples)\n",
        "#     while True:\n",
        "#         np.random.shuffle(indices)  # Shuffle the indices at the beginning of each epoch\n",
        "#         batch_indices = []\n",
        "#         for start in range(0, num_samples, batch_size):\n",
        "#             end = min(start + batch_size, num_samples)\n",
        "#             batch_indices.extend(indices[start:end])\n",
        "\n",
        "#             if len(batch_indices) == batch_size:\n",
        "#                 batch_X = np.zeros((batch_size, 224, 224, 3))\n",
        "#                 batch_y = np.zeros((batch_size,))\n",
        "#                 for i, idx in enumerate(batch_indices):\n",
        "#                     path = X.iloc[idx][\"path\"]\n",
        "#                     image = load_and_preprocess_image(path)\n",
        "#                     label = y.iloc[idx]\n",
        "#                     X_modified = X.drop('path', axis=1)\n",
        "#                     X_modified['path_id'] = range(1, len(X_modified) + 1)\n",
        "\n",
        "#                     input_data = pd.DataFrame([X_modified.iloc[idx]])\n",
        "#                     prediction = ensemble.predict(input_data)\n",
        "\n",
        "#                     batch_X[i] = image\n",
        "#                     batch_y[i] = prediction\n",
        "#                 yield batch_X, batch_y\n",
        "#                 batch_indices = []  # Reset batch_indices for the next batch\n",
        "\n",
        "#         # If there are remaining samples that don't form a full batch, process them as well\n",
        "#         if batch_indices:\n",
        "#             batch_X = np.zeros((len(batch_indices), 224, 224, 3))\n",
        "#             batch_y = np.zeros((len(batch_indices),))\n",
        "#             for i, idx in enumerate(batch_indices):\n",
        "#                 path = X.iloc[idx][\"path\"]\n",
        "#                 image = load_and_preprocess_image(path)\n",
        "#                 label = y.iloc[idx]\n",
        "#                 X_modified = X.drop('path', axis=1)\n",
        "#                 X_modified['path_id'] = range(1, len(X_modified) + 1)\n",
        "\n",
        "#                 input_data = pd.DataFrame([X_modified.iloc[idx]])\n",
        "#                 prediction = ensemble.predict(input_data)\n",
        "\n",
        "#                 batch_X[i] = image\n",
        "#                 batch_y[i] = prediction\n",
        "#             yield batch_X, batch_y\n",
        "\n",
        "\n",
        "# def load_and_preprocess_image(path):\n",
        "#     image = cv2.imread(path)\n",
        "#     if image is not None:\n",
        "#       image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#       image = cv2.resize(image, (224, 224))\n",
        "#       image = image / 255.0\n",
        "#     return image\n",
        "\n",
        "\n",
        "X_train = train_df.drop(columns=[\"label\", \"path_id\"])\n",
        "y_train = train_df[\"label\"]\n",
        "\n",
        "X_test = test_df.drop(columns=[\"label\", \"path_id\"])\n",
        "y_test = test_df[\"label\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOKwjzx6l_js"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def run_code():\n",
        "\n",
        "    num_classes = 10\n",
        "    base_model = EfficientNetB7(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Assuming X_train, y_train, X_test, y_test are your data arrays\n",
        "    batch_size = 4  # Adjust as needed\n",
        "\n",
        "    num_classes = 10\n",
        "    # One-hot encode the labels\n",
        "    y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
        "    y_test_encoded = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "    # Creating data generators\n",
        "    train_generator = image_generator(X_train, y_train, batch_size)\n",
        "    test_generator = image_generator(X_test, y_test, batch_size)\n",
        "\n",
        "    epochs = 10\n",
        "    accuracy_history = []\n",
        "    loss_history = []\n",
        "\n",
        "\n",
        "    epochs = 10\n",
        "    accuracy_history = []\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Using the generator to provide batches of data\n",
        "        history = model.fit(train_generator, epochs=epoch+1, validation_data=test_generator)\n",
        "\n",
        "        # Append the accuracy and loss lists for all epochs to history\n",
        "        accuracy_history.append(history.history['accuracy'][0])\n",
        "        loss_history.append(history.history['loss'][0])\n",
        "\n",
        "    print(\"Completed\")\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    test_loss, test_acc = model.evaluate(test_generator)\n",
        "    print(\"Test Loss:\", test_loss, \"Test accuracy:\", test_acc)\n",
        "\n",
        "    # Assuming test_generator provides batches of data\n",
        "    y_pred = model.predict(test_generator)\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    report = classification_report(y_test, y_pred_labels)\n",
        "    print(report)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_labels)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred_labels, average='macro')\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test_processed, y_pred[:, 1])\n",
        "\n",
        "    auc_pr = auc(recall, precision)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (AUC = %0.2f)' % auc_pr)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "    #plot accuracy\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(1, epochs + 1), accuracy_history, marker='o', linestyle='-')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.xticks(np.arange(1, epochs + 1))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    #plot loss\n",
        "    plt.plot(range(1, epochs + 1), loss_history, marker='o', linestyle='-')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.xticks(np.arange(1, epochs + 1))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    return 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBRyPOda2SVm",
        "outputId": "32a083c2-fa8f-40fa-f048-bbcf8ded884f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    653/Unknown - 5380s 8s/step - loss: 14.3723 - accuracy: 0.4118"
          ]
        }
      ],
      "source": [
        "run_code()\n",
        "\n",
        "after_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
        "memory_usage = after_memory - before_memory\n",
        "\n",
        "print(\"Memory usage:\", memory_usage, \"MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9LMl_QEhrkp"
      },
      "outputs": [],
      "source": [
        "def image_generator(X, y, batch_size=32):\n",
        "    num_samples = len(X)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)  # Shuffle the indices at the beginning of each epoch\n",
        "        batch_indices = []\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            end = min(start + batch_size, num_samples)\n",
        "            batch_indices.extend(indices[start:end])\n",
        "\n",
        "            if len(batch_indices) == batch_size:\n",
        "                batch_X = np.zeros((batch_size, 224, 224, 3))\n",
        "                batch_y = np.zeros((batch_size,))\n",
        "                for i, idx in enumerate(batch_indices):\n",
        "                    path = X.iloc[idx][\"path\"]\n",
        "                    image = load_and_preprocess_image(path)\n",
        "                    label = y.iloc[idx]\n",
        "                    X_modified = X.drop('path', axis=1)\n",
        "                    X_modified['path_id'] = range(1, len(X_modified) + 1)\n",
        "\n",
        "                    input_data = pd.DataFrame([X_modified.iloc[idx]])\n",
        "                    prediction = ensemble.predict(input_data)\n",
        "\n",
        "                    batch_X[i] = image\n",
        "                    batch_y[i] = prediction\n",
        "                yield batch_X, batch_y\n",
        "                batch_indices = []  # Reset batch_indices for the next batch\n",
        "\n",
        "        # If there are remaining samples that don't form a full batch, process them as well\n",
        "        if batch_indices:\n",
        "            batch_X = np.zeros((len(batch_indices), 224, 224, 3))\n",
        "            batch_y = np.zeros((len(batch_indices),))\n",
        "            for i, idx in enumerate(batch_indices):\n",
        "                path = X.iloc[idx][\"path\"]\n",
        "                image = load_and_preprocess_image(path)\n",
        "                label = y.iloc[idx]\n",
        "                X_modified = X.drop('path', axis=1)\n",
        "                X_modified['path_id'] = range(1, len(X_modified) + 1)\n",
        "\n",
        "                input_data = pd.DataFrame([X_modified.iloc[idx]])\n",
        "                prediction = ensemble.predict(input_data)\n",
        "\n",
        "                batch_X[i] = image\n",
        "                batch_y[i] = prediction\n",
        "            yield batch_X, batch_y\n",
        "\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = cv2.imread(path)\n",
        "    if image is not None:\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      image = cv2.resize(image, (224, 224))\n",
        "      image = image / 255.0\n",
        "    return image\n",
        "\n",
        "X_train = train_df.drop(columns=[\"label\", \"path_id\"])\n",
        "y_train = train_df[\"label\"]\n",
        "\n",
        "X_test = test_df.drop(columns=[\"label\", \"path_id\"])\n",
        "y_test = test_df[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKcTDqBVjC1W"
      },
      "outputs": [],
      "source": [
        "def run_code():\n",
        "    base_model = EfficientNetB7(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=(224, 224, 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    print(\"X_test\", X_test, \"y_test\", y_test)\n",
        "    train_generator = image_generator(X_train, y_train, 500)\n",
        "    test_generator = image_generator(X_test, y_test, 500)\n",
        "    print(\"test_generator\",test_generator)\n",
        "\n",
        "    X_train_processed, y_train_processed = next(train_generator)\n",
        "    X_test_processed, y_test_processed = next(test_generator)\n",
        "    print(\"X_test_processed\", len(X_test_processed), \"y_test_processed\", len(y_test_processed))\n",
        "\n",
        "    epochs = 5\n",
        "    accuracy_history = []\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        history = model.fit(X_train_processed, y_train_processed, epochs=epoch+1, validation_data=(X_test_processed, y_test_processed))\n",
        "        batch_size = 2\n",
        "        # history = model.fit(train_generator, steps_per_epoch=len(X_train_processed)//batch_size, epochs=epochs, validation_data=test_generator, validation_steps=len(X_test_processed)//batch_size)\n",
        "        # history = model.fit_generator(train_generator, steps_per_epoch=len(X_train) // batch_size, epochs=epochs, validation_data=test_generator, validation_steps=len(X_test) // batch_size)\n",
        "        # Access training accuracy and loss for all epochs\n",
        "        training_accuracy = history.history['accuracy'][0]\n",
        "        training_loss = history.history['loss'][0]\n",
        "\n",
        "        # Append the accuracy and loss lists for all epochs to history\n",
        "        accuracy_history.append(training_accuracy)\n",
        "        loss_history.append(training_loss)\n",
        "\n",
        "    # model.fit(X_train_processed, y_train_processed, validation_data=(X_test_processed, y_test_processed), epochs=10)\n",
        "\n",
        "    print(\"Completed\")\n",
        "    test_loss, test_acc = model.evaluate(X_test_processed, y_test_processed)\n",
        "    print(\"Test Loss:\",test_loss, \"Test accuracy:\", test_acc)\n",
        "    # Check the shape and data type of y_train_processed and y_test_processed\n",
        "\n",
        "    # Assuming y_test_processed contains binary labels (0 and 1)\n",
        "    y_pred = model.predict(X_test_processed)\n",
        "    y_pred_labels = y_pred.argmax(axis=1) # Convert probabilities to binary labels\n",
        "\n",
        "    # print(\"y_test_processed\", y_test_processed , \"y_pred_labels\", y_pred_labels, \"X_test_processed\", X_test_processed)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(y_test_processed, y_pred_labels)\n",
        "    print(report)\n",
        "    # Convert the data type of y_test_processed to integer\n",
        "    y_test_processed = y_test_processed.astype(int)\n",
        "\n",
        "    # Get predicted labels from the model\n",
        "    y_pred = model.predict(X_test_processed)\n",
        "    # print(\"y_pred\", y_pred, y_test_processed)\n",
        "    y_pred_labels = y_pred.argmax(axis=1)\n",
        "\n",
        "    # Generate the confusion matrix\n",
        "    cm = confusion_matrix(y_test_processed, y_pred_labels)\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"confusion_matrix\", cm)\n",
        "    # Calculate the F1 score\n",
        "    f1 = f1_score(y_test_processed, y_pred_labels, average='macro')\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "    #plot accuracy\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(1, epochs + 1), accuracy_history, marker='o', linestyle='-')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.xticks(np.arange(1, epochs + 1))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    #plot loss\n",
        "    plt.plot(range(1, epochs + 1), loss_history, marker='o', linestyle='-')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.xticks(np.arange(1, epochs + 1))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test_processed, y_pred[:, 1])\n",
        "\n",
        "    # Calculate area under the precision-recall curve (AUC-PR)\n",
        "    auc_pr = auc(recall, precision)\n",
        "\n",
        "    # Plot precision-recall curve\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (AUC = %0.2f)' % auc_pr)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "    return 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}